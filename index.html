<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>The hidden manifold distance for functional data</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">The hidden manifold distance for functional manifold data</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Paper</a>
</li>
<li>
  <a href="sim_functional_data.html">Functional manifold simulation scenarios</a>
</li>
<li>
  <a href="test_analytic_geodesic_distance.html">Verifying analytic geodesic distance derivation</a>
</li>
<li>
  <a href="README.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">The hidden manifold distance for functional data</h1>

</div>


<div id="abstract" class="section level1">
<h1>Abstract</h1>
<p>The clustering and classification of functional data, discretely-sampled data varying over a continuum, is frequently encountered in machine learning. While one can naively treat the data as vectors in Euclidean space, one may also regard each data point as a function. The challenge of analyzing infinite-dimensional functions can be ameliorated if the curves are smooth since the dimensionality is then only artificially high. This insight underpins functional data analysis, a subfield of statistics that studies functional data. For certain types of functional data such as classes of probability density functions and classes of warped curves of a common template function, there is further structure to exploit. Namely, when functional data lie on a manifold, the dimensionality at play might be even lower if the manifold is itself of low intrinsic dimension. <!-- Since these spaces are not vector spaces, even basic operations such as addition and subtraction require special consideration.--> This work addresses the estimation of pairwise geodesic distances between functional manifold data that are observed with noise and which may live near, rather than on, a manifold. <!-- This setting cannot be tackled by classic manifold learning techniques which require data to live exactly on a manifold.  --> <!-- We will show that estimation of the geodesic distance cannot easily be accomplished by a naive method such as smoothing followed by a shortest-path algorithm. --> The proposed methodology first sends the observed functional data to the hidden manifold, estimated using subspace-constrained mean-shift. Geodesic distances are subsequently calculated by employing shortest-path algorithms on this estimated manifold. Improved estimation of the pairwise geodesic distance has beneficial implications for many downstream tasks in functional data analysis which we illustrate in the case of distance-based functional classification.</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Many statistical and machine learning methods rely on some measure of distance. For example, clustering and classification of functional data, discretely-sampled data varying over a continuum, is a common machine learning task in which distance plays a crucial role. The successful development of functional data analysis, a branch of statistics dedicated to the analysis of infinite-dimensional smooth curves, demonstrates that employing the <span class="math inline">\(L^2\)</span> distance between the smoothed functional observations has advantages over the Euclidean distance between functional observations treated as vectors. In this work, we advocate that for functional manifold data, i.e. functional data that lie on a manifold, the geodesic distance is potentially even better than the <span class="math inline">\(L^2\)</span> distance, with benefits realized in downstream tasks such as distanced-based clustering and classification of functional manifold data. In other words, when the manifold hypothesis is plausible, e.g. classes of probability density functions and classes of warped curves of a common template function, clustering and classification using the goedesic distance may give better results than using the <span class="math inline">\(L_2\)</span> distance.</p>
<p>Let <span class="math inline">\(X_1,\ldots,X_n\)</span> be a sample of <span class="math inline">\(n\)</span> independant realizations of a random variable <span class="math inline">\(X\)</span> that takes value in the Hilbert space <span class="math inline">\(L^2([a,b],{\mathbb{R}})\)</span>. Suppose additionally that the function <span class="math inline">\(X\)</span> belongs to a Riemannian manifold <span class="math inline">\({\mathcal{M}}\subset L^2([a,b],{\mathbb{R}})\)</span> with Riemannian metric tensor <span class="math inline">\(g\)</span> which can be used to assign a metric on the manifold as follows <span class="citation">(Lin et al. 2014)</span>. For each point <span class="math inline">\(X\)</span> on the manifold, the Riemannian metric tensor <span class="math inline">\(g\)</span> has an inner product <span class="math inline">\(g_X\)</span> on the tangent space <span class="math inline">\(T_X {\mathcal{M}}\)</span>. The norm of a tangent vector <span class="math inline">\(V \in T_X {\mathcal{M}}\)</span> is defined as <span class="math display">\[||V|| = \sqrt{g_X(V,V)}.\]</span> The geodesic distance between two functions <span class="math inline">\(X_i,X_j\)</span> on the manifold <span class="math inline">\({\mathcal{M}}\)</span>, based on this metric tensor <span class="math inline">\(g\)</span>, is defined as <span class="math display">\[ d_{{\mathcal{M}}}(X_i,X_j):=\inf \{l(\gamma): \gamma \text{ is piecewise-smooth function from } [a,b] \text{ to } {\mathcal{M}}, \gamma(a) = X_i, \gamma(b) = X_j \}\]</span> where <span class="math display">\[ l(\gamma) := \int_a^b || \frac{\,d\gamma}{\,d t}(t) || \,dt\]</span> is the length of the curve <span class="math inline">\(\gamma\)</span>. ???Is it clear how <span class="math inline">\(l(\gamma)\)</span> depends on <span class="math inline">\(g\)</span>???</p>
<!-- ???insert example that shows pairwise geodesic distances give visually more interesting results than pairwise $L_2$ distance.??? -->
<!-- (gives examples and references for functional manifolds). -->
<!-- Before continuing, some diambiguities are called for. Contrast existing work on functional data where either the domain is a manifold or the range is a manifold, or both.  -->
<!-- There exist many nonlinear dimension reduction methods for manifolds embedded in Euclidean space, also known as manifold learning methods, which are particularly popular in computer vision. However, the success of these methods typically require the data to be observed with a high signal-to-noise ratio. (is it true? ref?)  -->
<p>Ideally, the functions <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are observed on a very dense domain grid with no measurement error. Then the estimation of the geodesic distance <span class="math inline">\(d_{\mathcal{M}}(X_i,X_j)\)</span> is straightfoward, e.g. a shortest-path algorithm such as Floyd-Warhsall <span class="citation">(Floyd 1962)</span> may be used. However, most shortest-path algorithms critically assume observations live on the manifold ???add reference??? and are thus likely to fail for functional data which are almost certainly observed with noise. <!-- A preprocessing step where the nosiy functional manifold data is smoothed is also likely to fail. This is because smoothing techniques in functional data analysis are designed for the $L_2$ recovery of the original curve, not to ensure the recovered functional versions of the data lie on or close to the functional manifold $\M$. This presents challenges to the application of many manifold learning techniques which assume fully observed nearly-noiseless inputs. --> <!-- Estimating the manifold $\M$ from a sample of functions observed discretely and with measurements is a challenging task (results depend on curvature of the manifold, need a lot of data). Even after recovering a functional version of the data, the chances are pretty high that $\tilde X_1,\ldots,\tilde X_n$ do not lie exactly on $\M$.  --> In this work, we put forth a technique for the recovery of the <span class="math inline">\(n{\times}n\)</span> matrix <span class="math inline">\(G\)</span> of pairwise geodesic distances, i.e.<br />
<span class="math display">\[
G(i,j)=G(j,i) = \begin{cases} 
      d_{{\mathcal{M}}} (X_i,X_j) &amp; \textrm{if $i\neq j$,} \\
      0 &amp; \textrm{otherwise.}
   \end{cases}
\]</span> In lieu of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>, we have access only to discretely-sampled noisy functional observations <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> that possibly lie off the true manifold.</p>
<!-- Pairwise distances are important for many downstream tasks, e.g.  classification, clustering, manifold learning (ISOMAP), nonparametric regression (kernel regression). Certainly, pairwise geodesic distances are legitimate objects of interest in themselves but our motivation is more general in nature. Specifically, we use pairwise distances as an illuminating example to advocate and promote the nonlinear perspective in FDA.  -->
<p>The work of <span class="citation">(Chen and Muller 2012)</span> was among the first in functional data analysis to consider the manifold hypothesis for functional data. A notion of the mean and variation of functional manifold data was introduced there. Of particular relevance to this work is their proposed P-ISOMAP procedure which adds a penalty to allow for noisy functional observations whereas the classic ISOMAP algorithm <span class="citation">(Tenenbaum, Silva, and Langford 2000)</span> assumes observations lie exactly on the manifold. <span class="citation">(Dimeglio et al. 2014)</span> also realized this drawback to ISOMAP and proposed a procedure we will call robust-ISOMAP that is less sensitive to outliers. We will discuss P-ISOMAP and robust-ISOMAP in more details in the simulation section.</p>
<!-- [@LinYao2017]  -->
</div>
<div id="proposed-method-for-estimating-geodesic-distances" class="section level1">
<h1>Proposed method for estimating geodesic distances</h1>
<p>Suppose that each curve <span class="math inline">\(X_i \in ({\mathcal{M}},g)\)</span> is observed with measurement error on a time grid <span class="math inline">\(T_i=(t_{i1},\ldots,t_{iK}), a \le t_{i1} &lt; \ldots &lt; t_{iK} \le b\)</span>, i.e. we observe a sample of <span class="math inline">\(K\)</span>-dimensional vectors <span class="math inline">\(Y_1,\ldots,Y_n\)</span> with <span class="math inline">\(Y_{ij} = X_i(t_{ij}) + \epsilon_{ij}\)</span>, where the random variables <span class="math inline">\(\epsilon_{ij}\)</span> have mean zero and are uncorrelated with each other. We assume that each observation grid <span class="math inline">\(T_1,\ldots,T_n\)</span> is dense, i.e. <span class="math inline">\(K\)</span> is large.</p>
We begin by converting the discretly-sampled functional observations into continuous ones. Let <span class="math inline">\(\tilde X_1,\ldots,\tilde X_n\)</span> denote the functional versions of the raw data obtained by some smoothing method. For example, we may employ spline smoothing ???add reference for spline smoothing??? to recover the functional versions of the raw data, i.e.<br />

<span class="math display">\[\begin{equation}\label{eq_spline_smoothing} 
\tilde X_i = \arg \min_{f\in C^2[0,1]}\left\{\sum_{j=1}^{K}\left(f(t_{ij})-Y_{ij}\right)^2+\lambda \|\partial^2_tf\|^2_{L^2}\right\},
\end{equation}\]</span>
<p>where <span class="math inline">\(\lambda&gt;0\)</span> is a tuning parameter controlling the smoothness of <span class="math inline">\(\tilde X_i\)</span>. The proposed methodology for estimating the pairwise geodesic distances <span class="math inline">\(\{ d_{\mathcal{M}}(X_i,X_j)\}_{i&gt;j}\)</span> is agnostic to the smoothing method employed.</p>
<p>Our method is based on the idea that the underlying functional manifold <span class="math inline">\({\mathcal{M}}\)</span> can be sufficiently well-recovered by the subspace-constrained mean-shift (SCMS) algorithm <span class="citation">(Ozertem and Erdogmus 2011)</span>. Let <span class="math inline">\(\hat M \subset L^2([a,b],\mathbb R)\)</span> denote the collection of basins of attraction of the estimated probability density function of <span class="math inline">\(X\)</span>. The SCMS algorithm sends each point <span class="math inline">\(\tilde X_i \in L^2([a,b],\mathbb R)\)</span> to a point in <span class="math inline">\(\hat M\)</span>. This destination is unique and we denote it <span class="math inline">\(\tilde X_i^{\hat {\mathcal{M}}}\)</span>. We expect the points <span class="math inline">\(\tilde X^{\hat {\mathcal{M}}}_1,\ldots,\tilde X^{\hat {\mathcal{M}}}_n \in \hat M\)</span> to lie close to the real manifold <span class="math inline">\({\mathcal{M}}\)</span>. Theoretical justificaton that <span class="math inline">\(\hat M\)</span> as estimated by SCMS is a reasonable surrogate for the true manifold <span class="math inline">\({\mathcal{M}}\)</span> can be found in <span class="citation">(Genovese et al. 2014)</span>.</p>
Before presenting our procedure, we need to first review the ISOMAP algorithm <span class="citation">(Tenenbaum, Silva, and Langford 2000)</span>, a three-step procedure that takes as input a set of points <span class="math inline">\(x_1,\ldots,x_n\)</span> in a submanifold <span class="math inline">\({\mathcal{M}}\)</span> of <span class="math inline">\({\mathbb{R}}^D\)</span> and produces an embedding in the space <span class="math inline">\({\mathbb{R}}^d\)</span> with <span class="math inline">\(d&lt;D\)</span> that preserves pairwise geodesic distances. The procedure is as follows.

<p>Note that shortest-path algorithms such as the Floyd-Warshall algorithm are used to find the smallest path  a weigthed graph but they do not produce the weighted graphs themselves. Both P-ISOMAP and robust-ISOMAP are modifications of ISOMAP in the sense that they change the construction of the weighted graph in step 1.<br />
<!-- (which is the difficult part, depend on number of neighbors, many method out there to calculate that graph in a robust way, P-ISOMAP is one of these). --></p>
<p>In what follows, we call IsoGeo the two-step procedure which performs a modified version of the first step of ISOMAP followed by the original second step of ISOMAP. The first step of IsoGeo constructs the weighted graph <span class="math inline">\(G\)</span> accoding to <span class="citation">Dimeglio et al. (2014)</span> so that graph is independent of the tuning parameter <span class="math inline">\(\epsilon\)</span>. Specifically, we begin by constructing the complete ???explain what complete means??? weighted graph <span class="math inline">\(G_c\)</span> with nodes corresponding to the observations <span class="math inline">\(x_1,\ldots,x_n\in {\mathbb{R}}^D\)</span>. Next, we obtain the minimal spanning tree associated with <span class="math inline">\(G_c\)</span> and denote its set of edges by <span class="math inline">\(E_s\)</span>. The graph <span class="math inline">\(G\)</span> is obtained by adding all edges <span class="math inline">\(e_{ij}\)</span> to <span class="math inline">\(E_s\)</span> for which the following condition is true: <span class="math display">\[ \overline{x_ix_j} \subset \bigcup_{i=1}^n B(x_i,\epsilon_i),\]</span> where <span class="math inline">\(B(x_i,\epsilon_i)\)</span> is the open ball of center <span class="math inline">\(x_i\)</span> and radius <span class="math inline">\(\epsilon_i = \max_{e_{ij}\in E_s}d_{ij}\)</span>, and <span class="math inline">\(\overline{x_ix_j}= \{x\in R^D \ | \ \exists \lambda \in [0,1], x=\lambda x_i + (1-\lambda)x_j \}\)</span>. Let the distance estimated by IsoGeo be denoted <span class="math inline">\(\text{IsoGeo}(x_i,x_j)\)</span>. Note that IsoGeo has no tuning parameters.</p>
<p>We are now ready to describe our procedure. Since SCMS is based on kernel density estimation, we first reduce the dimension of our data with multidimensional scaling before applying SCMS to avoid the curse of dimensionality.</p>
<!-- 1. [Susan: I think the smoothing step should be independent of our proposed methodology] Transform each vector $Y_i$ into a function $\tilde X_i$ by spline smoothing: -->
<!-- $$ \tilde X_i = \arg\min_{f\in C^2[0,1]}\left\{\sum_{j=1}^{K}\left(f(t_{ij})-Y_{ij}\right)^2+\lambda \|\partial^2_tf\|^2_{L^2}\right\}$$ -->
<!-- where $\lambda>0$ is a tuning parameter controlling the smoothness of $\tilde X_i$.  -->

<p>Our geodesic distance estimator is the <span class="math inline">\(n{\times}n\)</span> matrix <span class="math inline">\(\hat G\)</span> whose elements are given by <span class="math display">\[
\hat G(i,j)=\hat G(j,i) = \left\{ \begin{array}{ll}
 \text{IsoGeo}(\tilde X^{s,\hat {\mathcal{M}}}_i,\tilde X^{s,\hat {\mathcal{M}}}_j) &amp; \textrm{if $i\neq j$,}\\
 0 &amp; \textrm{otherwise.}
  \end{array} \right.
\]</span></p>
<p>There are two tuning parameters to our procedure. First, there is the dimension <span class="math inline">\(s\)</span> in step 1. In our simulations we try <span class="math inline">\(s \in \{1,2,3\}\)</span>. The other parameter is the bandwidth <span class="math inline">\(h\)</span> in the subspace constrained mean-shift. In our simulations we set <span class="math inline">\(h\)</span> according to the heuristic in Equation (A1) of <span class="citation">(Chen et al. 2015)</span>. In reality, depending on the downstream task, both <span class="math inline">\(s\)</span> and <span class="math inline">\(h\)</span> can be tuned in a data-adaptive way, say using cross-validation.</p>
<!-- ##############  -->
</div>
<div id="simulation-study" class="section level1">
<h1>Simulation study</h1>
<p>We perform a simulation study to ascertain the efficacy of our method for estimating pairwise geodesic distances. Three different metrics are used to assess the quality of a pairwise geodesic distance estimator.</p>
<p>The first metric assesses near-<span class="math inline">\(\epsilon\)</span> isometry, which we define as the percentage of estimated pairwise geodesic distances between <span class="math inline">\(1-\epsilon\)</span> and <span class="math inline">\(1+\epsilon\)</span> of the corresponding true pairwise geodesic distance. We form the receiver operating curve with <span class="math inline">\(\epsilon\)</span> on the <span class="math inline">\(x\)</span>-axis and near-<span class="math inline">\(\epsilon\)</span> isometry on the <span class="math inline">\(y\)</span>-axis. The metric is the area under the receiver operating curve.</p>
<p>The second metric <span class="math inline">\(||d-\hat d||/||d||\)</span> is the Frobenius norm of the estimation error matrix <span class="math inline">\(d-\hat d\)</span>, relative to the Frobenius norm of the true distance matrix <span class="math inline">\(d\)</span>. The third metric is the Pearson correlation coefficient between the upper diagonal of <span class="math inline">\(d\)</span> and <span class="math inline">\(\hat d\)</span>.</p>
<div id="alternative-estimators-of-geodesic-distance-marie" class="section level2">
<h2>Alternative estimators of geodesic distance [Marie]</h2>
<p>We compare the performance of our method to each of the following four methods. The first method, denoted RD for raw data, takes the naive approach of applying IsoGeo directly on the raw vectors <span class="math inline">\(Y_1,\ldots,Y_n \in R^K\)</span>. Note that this procedure is only sensible if all the grids <span class="math inline">\(T_1,\ldots,T_n\)</span> are the same. In the second method, we first smooth the discretely-sampled noisy functional data and then estimate their pairwise distance using the <span class="math inline">\(L^2\)</span> distance.</p>
<p>The third alternative, denoted SS for spline smoothing, applies IsoGeo on the smoothed version <span class="math inline">\(\tilde X_1,\ldots,\tilde X_n\)</span> of the raw data obtained by spline smoothing as described in (). <!--Transform each vector $Y_i$ into a function $\tilde X_i$ by spline smoothing. Apply IsoGeo on the vectors $\{(\tilde X_i(t_1), \ldots, X_i(t_K)\}_{i=1}^n$, where $t_1,\ldots,t_K$ is a regular grid of $[a,b]$, to obtain an estimator $\hat G_{\textrm{SS}}$.--> The final alternative we consider is P-ISOMAP, denoted pI, which is the two-step procedure developed in <span class="citation">Chen and Muller (2012)</span> designed to handle noisy functional data. In the first step, a penalty is incorporated to robustify the construction of the weighted graph. This is followed by the original second step of ISOMAP. <!--* **(Random Projection) RP**  same method as our but change step 2 by : obtain $s$-dimensional representation by random projection and setp 4 by obtain $\hat G$ using a ensemble method.--></p>
<p>We ran Chen and Muller’s P-ISOMAP separately in Matlab and tuned for the number of neighbors and the penalty parameter using the relative Frobenius assessment metric. We found that the penalty parameter chosen in this way was always zero, thus reducing hte P-ISOMAP procedure to standard ISOMAP. Given this, we did not make further comparison to P-ISOMAP in our simulation study.</p>
</div>
<div id="simulation-scenarios" class="section level2">
<h2>Simulation scenarios</h2>
<p>We shall consider three functional manifolds in our simulation study. As of yet, there is little work as to how to sample from a functional manifold. Even in the Euclidean case, it is not obvious how sampling should be done <span class="citation">(Diaconis, Holmes, and Shahshahani 2013)</span>. How to properly sample from a functional manifold could be interesting future work. For now, to safeguard against sampling unevenly on the functional manifold, we sample on a very concentrated measure for the intrinsic parameters.</p>
<p> <!-- Scenario 2 in sim_functional_data --> This scenario is modified from what is referred to as Manifold 2 in <span class="citation">(Chen and Muller 2012)</span> by fixing the variance of the normal density to be <span class="math inline">\(1\)</span>. We have <span class="math display">\[{\mathcal{M}}=  \left \{X_\beta: \beta \in [-1,1], t \in [a,b]\right \}\]</span> with the <span class="math inline">\(L_2\)</span> inner product as the metric tensor of <span class="math inline">\({\mathcal{M}}\)</span>, where <span class="math inline">\(X_\beta: [a,b] \to {\mathbb{R}}\)</span> is given by <span class="math inline">\(X_\beta(t) = \frac{1}{\sqrt{2\pi}} \exp{[-\frac{1}{2}(t-\beta)^2]}\)</span>. First, note that the “straight” line connecting <span class="math inline">\(X_{\beta_1}\)</span> and <span class="math inline">\(X_{\beta_2}\)</span> in <span class="math inline">\({\mathcal{M}}\)</span> does not always stay inside of <span class="math inline">\({\mathcal{M}}\)</span>. Thus the geodesic distance will not coincide with the <span class="math inline">\(L^2\)</span> distance.</p>
We set <span class="math inline">\(a=-4\)</span> and <span class="math inline">\(b=4\)</span> and sample <span class="math inline">\(\beta\)</span> according to a truncated standard normal with support on <span class="math inline">\([-1,1]\)</span>. The geodesic distance between the curves <span class="math inline">\(X_{\beta_1}\)</span> and <span class="math inline">\(X_{\beta_2}\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
d(X_{\beta_1},X_{\beta_2}) &amp;=&amp; \int_{\beta_1}^{\beta_2} \left \| \frac{d X_\beta (t)}{d\beta} \right\|_{L^2} d\beta \\
&amp;=&amp;  \int_{\beta_1}^{\beta_2} \sqrt{ \frac{1}{2\sqrt{\pi}} \int_{-4}^4 \frac{1}{\sqrt{\pi}} \exp\{-(t-\beta)^2\}(t-\beta)^2 dt  }  \  d\beta \\
&amp;=&amp;   \int_{\beta_1}^{\beta_2} \sqrt{ \frac{1}{2\sqrt{\pi}} \int_{-4}^4(t-\beta)^2 f(t) dt  }  \  d\beta, \textrm{ where $f$ is the density of a N$(\beta,1/2)$ }   \\
&amp;\approx&amp;  \int_{\beta_1}^{\beta_2}  \sqrt{ \frac{1}{2\sqrt{\pi}} \frac{1}{2}} \ d\beta \\
&amp;=&amp; (\beta_2-\beta_1) \frac{1}{2\pi^{1/4}},
\end{eqnarray*}\]</span>
<p>where the approximation comes from the fact that we are integrating on <span class="math inline">\([a,b]=[-4,4]\)</span> and not on <span class="math inline">\({\mathbb{R}}\)</span>. We can see this manifold is isometric, since the geodesic distance between <span class="math inline">\(X_{\beta_1}\)</span> and <span class="math inline">\(X_{\beta_2}\)</span> in <span class="math inline">\({\mathcal{M}}\)</span> is proportional to <span class="math inline">\(\beta_2-\beta_1\)</span>, the Euclidean distance between the <span class="math inline">\(\beta\)</span>’s.</p>
<p> <!-- Scenario 4 in sim_functional_data --> It was shown in <span class="citation">Joshi, Srivastava, and Jermyn (2007)</span> that the square root representation of probability density functions gives rise to a simple closed-form geodesic distance. They consider the manifold <span class="math display">\[ {\mathcal{M}}= \{ \psi:[0,1] \to {\mathbb{R}}: \psi \ge 0, \int_0^1 \psi^2(s) \,ds = 1 \}\]</span> with the metric tensor given by the Fisher-Rao metric tensor <span class="math display">\[ &lt;v_1,v_2&gt; = \int_0^1 v_1(s) v_2(s) \,ds \]</span> for two tangent vectors <span class="math inline">\(v_1,v_2 \in T_\psi({\mathcal{M}})\)</span>. Note that this concides with the <span class="math inline">\(L_2[0,1]\)</span> inner product. <span class="citation">Joshi, Srivastava, and Jermyn (2007)</span> showed that the geodesic distance between any two <span class="math inline">\(\psi_1\)</span> and <span class="math inline">\(\psi_2\)</span> in <span class="math inline">\({\mathcal{M}}\)</span> is simply <span class="math display">\[d_{\mathcal{M}}(\psi_1,\psi_2) = \cos^{-1}&lt;\psi_1,\psi_2&gt;.\]</span> We will specifically examine the square root of <span class="math inline">\(Beta(\alpha,\beta)\)</span> distributions which is supported on <span class="math inline">\([0,1]\)</span>. That is, <span class="math display">\[ M = \{ \psi_{\alpha,\beta}: 1 \le \alpha \le 5, 2 \le \beta \le 5\} \]</span> where <span class="math inline">\(\psi_{\alpha,\beta}: [0,1] \to {\mathbb{R}}\)</span> is the pdf of <span class="math inline">\(Beta(\alpha,\beta)\)</span>. We sample <span class="math inline">\(\alpha\)</span> according to a truncated normal with mean 3 and variance 0.09 with support on <span class="math inline">\([1,5]\)</span>. We sample <span class="math inline">\(\beta\)</span> according to a truncated normal with mean 3.5 and variance 0.09 with support on <span class="math inline">\([2,5]\)</span>.</p>
<p> <!-- Scenario 5 in sim_functional_data --> This is based on Equations 17 and 18 in <span class="citation">Kneip and Ramsay (2008)</span> but with <span class="math inline">\(z_{i1}, z_{i2}\)</span> both set to <span class="math inline">\(1\)</span>. (Note that Equation 17 has a typo where the exponentials are missing negative signs.) Let <span class="math inline">\(X_\alpha(t) = \mu(h_\alpha(t))\)</span> be defined on <span class="math inline">\([-3,3]\)</span> where <span class="math display">\[ \mu(t) = \exp\{(t-1.5)^2/2\} + \exp\{(t+1.5)^2/2\}\]</span> and <span class="math display">\[ h_\alpha(t) = 6 \frac{ \exp\{\alpha(t+3)/6\} - 1}{\exp\{\alpha\}-1}\]</span> if <span class="math inline">\(\alpha \ne 0\)</span> and <span class="math inline">\(h_\alpha(t) = t\)</span> otherwise.</p>
<p>Consider the manifold <span class="math display">\[ M = \{X_\alpha: -1 \le \alpha \le 1\}.\]</span> We sample <span class="math inline">\(\alpha\)</span> according to a truncated standard normal with support on <span class="math inline">\([-1,1]\)</span>. The geodesic distance is then <span class="math display">\[ d(X_{\alpha_1},X_{\alpha_2}) = \int_{\alpha_1}^{\alpha_2} \left \| \frac{d X_\alpha (t)}{d\alpha} \right\|_{L^2} d\alpha\]</span> which is estimated using numerical integration.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>???Still need to describe parameters under study: SNR, denseness of observations (<span class="math inline">\(K_{observed}\)</span>). Describe parameters that are fixed: <span class="math inline">\(K_smooth\)</span> and com_grid. Describe tuning parameters for our method: <span class="math inline">\(s\)</span> and <span class="math inline">\(h\)</span>.</p>

<!--

## The cost of employing proposed method when manifold is flat [Susan/Marie?]

Suppose we take scenario 1 where the geodesic distance coincides with the $L_2$ distance. Then what is the cost of employing our distance estimator compared to smoothing the data and doing numerical integration to find the $L_2$ distance?

-->
</div>
</div>
<div id="distance-based-functional-classification-marie" class="section level1">
<h1>Distance-based functional classification [Marie]</h1>
<p>In this section, we explore whether our geodesic distance estimator has benefits for downstream analysis task. There are many tasks we could consider here such as distance-based nonparametric regression and distanced-based functional clustering, but we will focus on distance-based functional classification. It must be noted that while curve alignment, also known as curve registration, is necessarily performed as a preprocessing technique prior to clustering and classification, our geodesic distance estimator allows one to forsake this step.</p>
<p>For simplicity, assume the task is binary classification. Associated to each functional object <span class="math inline">\(x\)</span> is a binary <span class="math inline">\(y\)</span> indicating class membership. Consider the classifier proposed in Ferraty and Vieu (2003,2006) which is a functional version of the Nadaraya-Watson kernel estimator of class membership probabilities: <span class="math display">\[
\hat p(y = 0 | x) \frac{ \sum_{i=1}^n K[h^{-1} d(x,x_i)] 1(y_i = 0) }{ \sum_{i=1}^n K[h^{-1} d(x,x_i)] }
\]</span> We shall compare our method to using <span class="math inline">\(L_2\)</span> distance, possibly weighted, and with curve registration already accomplished. Describe alternative methods in detail.</p>
<p>The bandwidth in the classifier should be tuned individually for each method. Also we might need to tune MDS dimension <span class="math inline">\(s\)</span> since in real data, the dimension of the manifold might be much higher than encountered in the simulation scenarios where it never goes above 2.</p>
Datasets used by functional classification papers

Datasets used in functional manifold papers

<!-- ## Distance-based functional clustering -->
<!-- Describe the $k$-medoids clustering method. -->
<!-- ### Simulated data -->
<!-- ###  Real data -->
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-ChenMuller2012">
<p>Chen, Dong, and Hans-Georg Muller. 2012. “NONLINEAR Manifold Representations for Functional Data.” <em>The Annals of Statistics</em> 40 (1). Institute of Mathematical Statistics: 1–29. <a href="http://www.jstor.org/stable/41713625" class="uri">http://www.jstor.org/stable/41713625</a>.</p>
</div>
<div id="ref-ChenHo2015">
<p>Chen, Y., S. Ho, P. E. Freeman, C. R. Genovese, and L. Wasserman. 2015. “Cosmic Web Reconstruction Through Density Ridges: Method and Algorithm.” <em>Monthly Notices of the Royal Astronomical Society</em> 454 (1): 1140–56. doi:<a href="https://doi.org/10.1093/mnras/stv1996">10.1093/mnras/stv1996</a>.</p>
</div>
<div id="ref-Diaconis2013">
<p>Diaconis, Persi, Susan Holmes, and Mehrdad Shahshahani. 2013. “Sampling from a Manifold.” In <em>Advances in Modern Statistical Theory and Applications: A Festschrift in Honor of Morris L. Eaton</em>, Volume 10:102–25. Collections. Beachwood, Ohio, USA: Institute of Mathematical Statistics. doi:<a href="https://doi.org/10.1214/12-IMSCOLL1006">10.1214/12-IMSCOLL1006</a>.</p>
</div>
<div id="ref-Dimeglio2014">
<p>Dimeglio, Chloe, Santiago Gallon, Jean-Michel Loubes, and Elie Maza. 2014. “A Robust Algorithm for Template Curve Estimation Based on Manifold Embedding.” <em>Comput. Stat. Data Anal.</em> 70 (February). Amsterdam, The Netherlands, The Netherlands: Elsevier Science Publishers B. V.: 373–86. doi:<a href="https://doi.org/10.1016/j.csda.2013.09.030">10.1016/j.csda.2013.09.030</a>.</p>
</div>
<div id="ref-Floyd1962">
<p>Floyd, Robert W. 1962. “Algorithm 97: Shortest Path.” <em>Commun. ACM</em> 5 (6). New York, NY, USA: ACM: 345. doi:<a href="https://doi.org/10.1145/367766.368168">10.1145/367766.368168</a>.</p>
</div>
<div id="ref-Genovese2014">
<p>Genovese, Christopher R., Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. 2014. “NONPARAMETRIC Ridge Estimation.” <em>The Annals of Statistics</em> 42 (4). Institute of Mathematical Statistics: 1511–45. <a href="http://www.jstor.org/stable/43556332" class="uri">http://www.jstor.org/stable/43556332</a>.</p>
</div>
<div id="ref-Joshi2007">
<p>Joshi, S., A. Srivastava, and I.H. Jermyn. 2007. “Riemannian Analysis of Probability Density Functions with Applications in Vision.” In <em>2007 Ieee Conference on Computer Vision and Pattern Recognition ; Proceedings</em>, 1664–71. Piscataway, NJ: IEEE. <a href="http://dro.dur.ac.uk/18442/" class="uri">http://dro.dur.ac.uk/18442/</a>.</p>
</div>
<div id="ref-Kneip2008">
<p>Kneip, Alois, and James O Ramsay. 2008. “Combining Registration and Fitting for Functional Models.” <em>Journal of the American Statistical Association</em> 103 (483). Taylor &amp; Francis: 1155–65. doi:<a href="https://doi.org/10.1198/016214508000000517">10.1198/016214508000000517</a>.</p>
</div>
<div id="ref-Lin2014">
<p>Lin, Binbin, Ji Yang, Xiaofei He, and Jieping Ye. 2014. “Geodesic Distance Function Learning via Heat Flow on Vector Fields.” In <em>Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32</em>, II–145–II–153. ICML’14. Beijing, China: JMLR.org. <a href="http://dl.acm.org/citation.cfm?id=3044805.3044909" class="uri">http://dl.acm.org/citation.cfm?id=3044805.3044909</a>.</p>
</div>
<div id="ref-Ozertem2011">
<p>Ozertem, Umut, and Deniz Erdogmus. 2011. “Locally Defined Principal Curves and Surfaces.” <em>J. Mach. Learn. Res.</em> 12 (July). JMLR.org: 1249–86. <a href="http://dl.acm.org/citation.cfm?id=1953048.2021041" class="uri">http://dl.acm.org/citation.cfm?id=1953048.2021041</a>.</p>
</div>
<div id="ref-Tenenbaum2000">
<p>Tenenbaum, Joshua B., Vin de Silva, and John C. Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” <em>Science</em> 290 (5500). American Association for the Advancement of Science: 2319–23. doi:<a href="https://doi.org/10.1126/science.290.5500.2319">10.1126/science.290.5500.2319</a>.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
