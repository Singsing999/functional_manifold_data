---
title: "Report"
output:
  html_document:
    df_print: paged
---

# Introduction

The classical setup in FDA is to assume that we observe a sample of $n$ independant realizations $X_1,\ldots,X_n$ of a random variable $X$ that takes value in the Hilbert space $L^2([a,b],\R)$. The first steps of a classical approach to analyse such data are to first obtain a functional version $\tilde X_1,\ldots,\tilde X_n$ of the raw data (by either smoothing or using PACE) and then perform FPCA to reduce the dimension of the data.


However, it might very well happen that the function $X$ belongs to $\M \subset L^2([a,b],\R)$ a low-dimensional nonlinear manifold (gives examples and references). If it is indeed the case, it is no longer appropriate to use the norm induced by the $L^2$ inner product to calculate the distance between two curves. Explain why. One should instead use the geodesic distance $d_\M$ induced by $\M$ which takes into account the intrinsic structure of $\M$.

Before continuing, some diambiguities are called for. Contrast existing work on functional data where either the domain is a manifold or the range is a manifold, or both. 

Now back to functional data living in a low-dimensional manifold embedded in the ambient Hilbert space. There exist many nonlinear dimension reduction methods in the literature, also known as manifold learning methods, which are particularly popular in computer vision. However, the success of these methods typically require the data to be observed with a high signal-to-noise ratio. (is it true? ref?) This is a luxury that FDA cannot always afford. 

Though powerful techniques exist for smoothing/interpolating discretely observed noisy functional data, the recovered functional versions of the data, $\tilde X_1,\ldots,\tilde X_n$, may not lie even close to the functional manifold $\M$. This presents challenges to the application of many manifold learning techniques which assume fully observed nearly-noiseless inputs.

% the following two sentences are not related to each other
% Estimating the manifold $\M$ from a sample of functions observed discretely and with measurements is a challenging task (results depend on curvature of the manifold, need a lot of data). Even after recovering a functional version of the data, the chances are pretty high that $\tilde X_1,\ldots,\tilde X_n$ do not lie exactly on $\M$. 

With this challenge in mind, we put forth a technique for the specific task of estimating pairwise geodesic distances $\{ d_\M (X_i,X_j\}_{i>j}$ when we have access only to discretely-observed noisy functional observations. Say why pairwise distances important, e.g.  classification, clustering, manifold learning (ISOMAP), nonparametric regression (kernel regression). Certainly, pairwise geodesic distances are legitimate objects of interest in themselves but our motivation is more general in nature. Specifically, we use pairwise distances as an illuminating example to advocate and promote the nonlinear perspective in FDA. 

???insert example that shows pairwise geodesic distances give visually more interesting results than pairwise $L_2$ distance. like the Berkeley growth curves, MDS based on pairwise geodesic versus MDS based on pairwise $L_2$. The former separates the genders better? Something like this, but hopefully a different dataset.???

Briefly outline the methodology: MDS plus density ridge are used in a cool way to estimate (consistently?, let's hope..) pairwise geodesic distances. 

Along the way, we introduce a new tool to assess pairwise geodesic distance estimation. This is the ROC curve with $\epsilon$ on the $x$-axis and degree to which near-$\epsilon$ isometry holds, i.e.\ the percentage of estimated pairwise distances between $1-\epsilon$ and $1+\epsilon$ of the truth pairwise distance.  

# Proposed method for estimating geodesic distances

Let $X_1,\ldots,X_n$ be independent realizations of a random function $X\in\M \subset L^2([a,b],\R)$, where $\M$ is an unknown manifold of dimension $d$. Suppose that each curve $X_i$ is observed on a grid $T_i=(t_{i1},\ldots,t_{iK})$ and with measurements errors, i.e. that we observe a sample of $K$-dimensional vectors $Y_1,\ldots,Y_n$ with $Y_{ij} = X_i(t_{ij}) + \epsilon_{ij}$, where the random variables $\epsilon_{ij}$ are of mean zero and uncorrelated with each other. We assume that the grids $T_1,\ldots,T_n$ are dense.

Our method is described by the following steps:

1. Transform each vector $Y_i$ into a function $\tilde X_i$ by spline smoothing:
$$ \tilde X_i = \arg\min_{f\in C^2[0,1]}\left\{\sum_{j=1}^{K}\left(f(t_{ij})-Y_{ij}\right)^2+\lambda \|\partial^2_tf\|^2_{L^2}\right\}$$
where $\lambda>0$ is a tuning parameter controlling the smoothness of $\tilde X_i$. 
2. Obtain a $s$-dimensional representation $\tilde X^s_1,\ldots,\tilde X^s_n$ of the functions $\tilde X_1,\ldots,\tilde X_n$ that "preserves" the pairwise $L^2$ distances by using MDS.
3. Obtain $\tilde X^{s,\hat \M}_1,\ldots,\tilde X^{s,\hat \M}_n$ a "projection" of $\tilde X^s_1,\ldots,\tilde X^s_n$ onto a ridge $\hat \M$ which is computed with a mean shift algorithm.
4. Use  IsoGeo to approximate the pairwise geodesic distances $\{d_{\hat \M}(\tilde X^{s,\hat \M}_i,\tilde X^{s,\hat \M}_j)\}_{i>j}$ and define the $n\times n $ matrix $\hat G$ as
$$
\hat G(i,j)=\hat G(j,i) = \left\{ \begin{array}{ll}
 d_{\hat \M}(\tilde X^{s,\hat \M}_i,\tilde X^{s,\hat \M}_j) & \textrm{if $i\neq j$,}\\
 0 & \textrm{otherwise.}
  \end{array} \right.
$$


Since the ridge estimation obtained from noisy measurements of a manifold should approximate well the manifold (ref. Genevose and al. 2014), we expect the points $\tilde X^{s,\hat \M}_1,\ldots,\tilde X^{s,\hat \M}_n$ to lie close to the real manifold $\M$ and then $d_{\hat \M}$ to be close to $d_\M$. Ridge estimation suffers from the curse of dimensionality, this is why we first reduce the dimension of our data with MDS and then apply the shift-mean algorithm to estimate the ridge.

# Comparison to other methods

Susan: why are we describing Isomap as the first sentence in this Simulation section? Shouldn't we begin by describing the goal. Something like: We perform a simulation study to ascertain the efficacy of our method for estimating pairwise geodesic distances for discretely-observed noisy functional data, compared to various alternative approaches.

MH : Sure, the introduction sentence was missing and we can also put the description of IsoMAP somewhere else.

Susan question: Also, isn't the real name for IsoGeo Floyd-Warshall or Dijkstra’s? These are the two options in python scikitlearn's isomap routine, where the default is actually automatic selection. 

MH : Floyd or Dijkstra are methods to find the smallest path given a weigthed graph but they are not related to how to calculate that graph (which is the difficult part, depend on number of neighbors, many method out there to calculate that graph in a robust way, p-isomap is one of these). I wanted to introduce that IsoGeo to avoid confusion but maybe it was not clear, I changed a bit the description.

Isomap is a three steps procedure that takes as input a set of points $x_1,\ldots,x_n\in \R^D$ and produces an embedding of the input data in the space $\R^d$ with $d<D$, that preserves pairwise geodesic distances. The first step consists in constructing a weighted graph $G$ with nodes corresponding to the input data and for which the weight of an edge between two nodes $x_i$ and $x_j$ is equal to $||x_i-x_j ||_{\R^D}$. The graph $G$ is constructed such that a node is connected to a fix number $N$ of neighbors or to all nodes that are at a distance smaller than a given value $\epsilon$. In the second step, the pairwise geodesic distances $d_\M(x_i,x_j)$
are estimated based on $G$. The geodesic distance between two nodes is simply the length of the shortest path in $G$ between these two nodes, and can be calculated easily with Floyd-Warshall or Dijkstra’s algorithms. The last step consists in using MDS on the matrix of pairwise geodesic distances obtained in the first step in order to obtain an embedding in $\R^d$. 

%In the sequel, we use the first step of Isomap to estimate pairwise geodesic distances and we call this procedure IsoGeo.
In the sequel, we call IsoGeo the procedure of estimating pairwise geodesic distances with the two first steps of Isomap.

We compare our method to the following ones:
* [\bf RD] Apply IsoGeo on the raw data $Y_1,\ldots,Y_n$ to obtain an estimator $\hat G_{\textrm{RD}}$. Note that this procedure can only be used if the grid $T_i$ is the same for each $i$.
* [\bf SS] Transform each vector $Y_i$ into a function $\tilde X_i$ by spline smoothing. Apply IsoGeo on the vectors $\{(\tilde X_i(t_1), \ldots, X_i(t_K)\}_{i=1}^n$, where $t_1,\ldots,t_K$ is a regular grid of $[a,b]$, to obtain an estimator $\hat G_{\textrm{SS}}$.
* [\bf pI] Transform each vector $Y_i$ into a function $\tilde X_i$ by spline smoothing. Calculate the weighted graph of step I of Isomap with a penalty (describe the penalty of Chen and Muller). Apply step II of Isomap to obtain $\hat G_{\textrm{pI}}$.
 [\bf RP] same method as our but change step 2 by : obtain $s$-dimensional representation by random projection and setp 4 by obtain $\hat G$ using a ensemble method.

# TODOs

* Clean up the writing in this notebook. Add code and graphs where helpful.
* Related to tuning parameter
    + Choose the number of neighbors $N$ in IsoGeo.
    + Choose projection dimension for avoiding curse of dimensionality in performing density ridge scms routine
    + Choose the bandwidth $h$ in scms.
* Related to spartanSim.R simulations results
    + Why do some Monte Carlo simulations fail?
    + Is P-isomap always choosing zero penalty automatically?
    + Shouldn’t the smoothing method be doing better? Thought the smooth was close to the true curve.
* Next time we submit spartanSim.R, make sure to hold dataset constant for different projection dimensions
* Read Marie's suggested paper on [robust ISOMAP](https://arxiv.org/abs/1306.3373)
* Revisit Mueller manifold examples. Need to write down some differential equations to solve the geodesic minimisation problem?
